{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandoordonez/deeplearning/blob/main/Happy_grid_search_Tunning_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxvYfYfceOG3"
      },
      "source": [
        "# **Grid Search**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXECT6v9TJHW"
      },
      "source": [
        "Basado en el proyecto de  Angélica Corrales Quevedo - Santiago Trochez Velasco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEU42mXPcmx"
      },
      "source": [
        "## 1 - Paquetes ##\n",
        "\n",
        "Primero vamos a importar los datasets necesarios para el proyecto, los cuales son los siguentes:\n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08SaY22bPcmy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszAJf8fPcmy"
      },
      "source": [
        "Luego, **definimos una función** que nos permita cargar los datos de entrenamiento y prueba del dataset de personas felices desde Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgux9anrPcmy"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    train_dataset = h5py.File('/content/drive/MyDrive/IA II/train_happy.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('/content/drive/MyDrive/IA II/test_happy.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oxqf9tzPcmy"
      },
      "source": [
        "## 2 - Carga del dataset y revisión de datos##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4je6WCMPcmy"
      },
      "source": [
        "Para empezar, otorgamos permisos a Google Drive para poder obtener el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWIym4BDPcmy",
        "outputId": "c1844ed5-8c78-47a0-f076-429f9c6a6154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywW43Lk1Pcmz"
      },
      "source": [
        "Y luego, cargamos el dataset en variables de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilWif9PDPcmz"
      },
      "outputs": [],
      "source": [
        "# Loading the data (happy/non-happy)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hFkYC7zzotO",
        "outputId": "7399220a-f630-426e-8116-33a18d62b243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_set_x_flatten shape: (600, 12288)\n",
            "train_set_y shape: (1, 600)\n",
            "test_set_x_flatten shape: (150, 12288)\n",
            "test_set_y shape: (1, 150)\n",
            "sanity check after reshaping: [178 193  82 188 207]\n"
          ]
        }
      ],
      "source": [
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], train_set_x_orig.shape[1] * train_set_x_orig.shape[2] * train_set_x_orig.shape[3])\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], test_set_x_orig.shape[1] * test_set_x_orig.shape[2] * test_set_x_orig.shape[3])\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZTZpJNP5nM"
      },
      "source": [
        "Ahora normalizamos los vectores dividiéndolos por distribución del **RGB**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3jlgD4GzpaL"
      },
      "outputs": [],
      "source": [
        "train_set_x = train_set_x_flatten/255.\n",
        "test_set_x = test_set_x_flatten/255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeE0U9V1RmXL"
      },
      "source": [
        "# Crea el modelo dinámicamente\n",
        "\n",
        "def create_model():\n",
        " ...\n",
        " return model\n",
        "\n",
        "model = KerasClassifier(model=create_model)\n",
        "\n",
        "# Puede recibir argumentos\n",
        "\n",
        "def create_model(dropout_rate=0.0):\n",
        " ...\n",
        " return model\n",
        "\n",
        "model = KerasClassifier(model=create_model, dropout_rate=0.2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKYrlZ1-UfzP",
        "outputId": "19264109-83b5-4589-88e8-eb15109c54b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv2iWN6-R4AM"
      },
      "outputs": [],
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "# Here you will evaluate a suite of different mini-batch sizes from 10 to 100 in steps of 20.\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten, Activation\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t89iUSKbR4Db"
      },
      "outputs": [],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " # Compile model\n",
        " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNW4xxUiU0z4"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "model = KerasClassifier(model=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "param_grid = dict(batch_size = batch_size, epochs= epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z8VtAFOU7-T",
        "outputId": "b86a3a17-5e80-4f94-d05b-9b1758db5140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.968333 using {'batch_size': 40, 'epochs': 100}\n",
            "0.846667 (0.047140) with: {'batch_size': 10, 'epochs': 10}\n",
            "0.946667 (0.022485) with: {'batch_size': 10, 'epochs': 50}\n",
            "0.888333 (0.077924) with: {'batch_size': 10, 'epochs': 100}\n",
            "0.896667 (0.023921) with: {'batch_size': 20, 'epochs': 10}\n",
            "0.863333 (0.070040) with: {'batch_size': 20, 'epochs': 50}\n",
            "0.943333 (0.030641) with: {'batch_size': 20, 'epochs': 100}\n",
            "0.850000 (0.014142) with: {'batch_size': 40, 'epochs': 10}\n",
            "0.951667 (0.012472) with: {'batch_size': 40, 'epochs': 50}\n",
            "0.968333 (0.008498) with: {'batch_size': 40, 'epochs': 100}\n",
            "0.770000 (0.035590) with: {'batch_size': 60, 'epochs': 10}\n",
            "0.963333 (0.013123) with: {'batch_size': 60, 'epochs': 50}\n",
            "0.956667 (0.015456) with: {'batch_size': 60, 'epochs': 100}\n",
            "0.780000 (0.014720) with: {'batch_size': 80, 'epochs': 10}\n",
            "0.935000 (0.021602) with: {'batch_size': 80, 'epochs': 50}\n",
            "0.960000 (0.007071) with: {'batch_size': 80, 'epochs': 100}\n",
            "0.713333 (0.058926) with: {'batch_size': 100, 'epochs': 10}\n",
            "0.930000 (0.028577) with: {'batch_size': 100, 'epochs': 50}\n",
            "0.941667 (0.018856) with: {'batch_size': 100, 'epochs': 100}\n"
          ]
        }
      ],
      "source": [
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))#leer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2YsnezxeeDT"
      },
      "source": [
        "Según el grid search, los mejores parámetros a usar son un **batch=40** y **epochs=100**, por lo que en los siguientes **grid search**, usaremos los mismos parámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4y02uRBmTbN"
      },
      "source": [
        "# How to Tune the Training Optimization Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqABjcPgfMKn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten, Activation\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O63zwkQOt9bI",
        "outputId": "6924ffc9-5a55-4e24-9c76-43e0761b1313"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.966667 using {'model__optimizer': 'Nadam'}\n",
            "0.930000 (0.008165) with: {'model__optimizer': 'SGD'}\n",
            "0.938333 (0.008498) with: {'model__optimizer': 'RMSprop'}\n",
            "0.916667 (0.033993) with: {'model__optimizer': 'Adagrad'}\n",
            "0.801667 (0.018856) with: {'model__optimizer': 'Adadelta'}\n",
            "0.928333 (0.022485) with: {'model__optimizer': 'Adam'}\n",
            "0.960000 (0.004082) with: {'model__optimizer': 'Adamax'}\n",
            "0.966667 (0.006236) with: {'model__optimizer': 'Nadam'}\n"
          ]
        }
      ],
      "source": [
        "def create_model(optimizer='adam'):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " # Compile model\n",
        " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=40, verbose=0)\n",
        "# define the grid search parameters\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(model__optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-yMEj8w7SP"
      },
      "source": [
        "# How to Tune Learning Rate and Momentum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyPeWuMqt9l-",
        "outputId": "68bac18e-9268-4b1f-ae1c-8efc8cf04986"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6663\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5975\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5925\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5185\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4848\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4398\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4086\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3966\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3878\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3610\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3381\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3442\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3039\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2921\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2868\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2723\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2584\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2375\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2373\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2227\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2551\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2332\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2155\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1875\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1857\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1858\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1755\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1778\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1653\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1565\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1587\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1553\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1688\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1494\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1432\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1388\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1406\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1347\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1345\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1287\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1364\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1249\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1255\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1341\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1169\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1134\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1106\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1084\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1096\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1064\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1056\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1068\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1080\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0985\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1185\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0940\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0902\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0946\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0862\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0939\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0968\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0895\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0805\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0803\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1166\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0796\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0824\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0992\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0806\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0795\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0746\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0810\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0726\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0730\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0788\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0781\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0745\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0716\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0671\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0672\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0723\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0629\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0702\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0666\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0752\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0658\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0620\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0702\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0588\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0601\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0832\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0609\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0559\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0565\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0621\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0579\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0654\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0614\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0560\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0557\n",
            "Best: 0.958333 using {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
            "0.920000 (0.017795) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
            "0.915000 (0.010801) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.2}\n",
            "0.938333 (0.015456) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.4}\n",
            "0.945000 (0.010801) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.6}\n",
            "0.958333 (0.006236) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
            "0.958333 (0.002357) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.9}\n",
            "0.921667 (0.032998) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
            "0.853333 (0.122837) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.2}\n",
            "0.955000 (0.008165) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.4}\n",
            "0.656667 (0.218034) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.6}\n",
            "0.640000 (0.197990) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.8}\n",
            "0.586667 (0.122565) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.9}\n",
            "0.498333 (0.002357) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.0}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.2}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.4}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.6}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.8}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.9}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.0}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.2}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.4}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.6}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.8}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.9}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.0}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.2}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.4}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.6}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.8}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.9}\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", optimizer=\"SGD\", epochs=100, batch_size=40, verbose=1)\n",
        "# define the grid search parameters\n",
        "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjdq_BWQsAgJ",
        "outputId": "7ca0c2b5-216e-4bb8-fb92-ff8a45862f14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.960000 using {'optimizer__learning_rate': 0.001}\n",
            "0.960000 (0.014720) with: {'optimizer__learning_rate': 0.001}\n",
            "0.498333 (0.002357) with: {'optimizer__learning_rate': 0.01}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.1}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.2}\n",
            "0.500000 (0.000000) with: {'optimizer__learning_rate': 0.3}\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", optimizer=\"Nadam\", epochs=100, batch_size=40, verbose=0)\n",
        "# define the grid search parameters\n",
        "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "param_grid = dict(optimizer__learning_rate=learn_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU97PLDZx5LK"
      },
      "source": [
        "#How to Tune the Neuron Activation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBtoT_vt9sR",
        "outputId": "1b1d2730-b72c-481f-ae44-9ad8c35f3a8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.975000 using {'model__activation': 'hard_sigmoid'}\n",
            "0.966667 (0.006236) with: {'model__activation': 'softmax'}\n",
            "0.945000 (0.028577) with: {'model__activation': 'softplus'}\n",
            "0.965000 (0.007071) with: {'model__activation': 'softsign'}\n",
            "0.966667 (0.009428) with: {'model__activation': 'relu'}\n",
            "0.950000 (0.007071) with: {'model__activation': 'tanh'}\n",
            "0.965000 (0.018708) with: {'model__activation': 'sigmoid'}\n",
            "0.975000 (0.004082) with: {'model__activation': 'hard_sigmoid'}\n",
            "0.961667 (0.002357) with: {'model__activation': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Nadam\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(activation='relu'):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " # Compile model\n",
        " optimizer = Nadam(learning_rate=0.001)\n",
        " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=40, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "param_grid = dict(model__activation=activation)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMgftXqNyfu3"
      },
      "source": [
        "# How to Tune Dropout Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vkbw4Qwye53",
        "outputId": "49de9537-c259-4c31-b4a0-70459d979f60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.651667 using {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
            "0.648333 (0.209775) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
            "0.650000 (0.212132) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
            "0.651667 (0.214489) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
            "0.651667 (0.214489) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
            "0.500000 (0.000000) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n"
          ]
        }
      ],
      "source": [
        "from keras.constraints import MaxNorm\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(dropout_rate, weight_constraint):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(64, input_shape=(64*64*3,), activation='hard_sigmoid', kernel_constraint=MaxNorm(weight_constraint)))\n",
        " model.add(Dropout(dropout_rate))\n",
        " model.add(Dense(32, activation='relu'))\n",
        " model.add(Dropout(dropout_rate))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " # Compile model\n",
        " optimizer = Nadam(learning_rate=0.001)\n",
        " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=40, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
        "\n",
        "#param_grid = dict(model__dropout_rate=dropout_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THAPOPp89Hsa"
      },
      "source": [
        "## Tuneando con número de neuronas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFIsBExtky9n",
        "outputId": "ad435f91-563f-4c12-bbe9-3ac6e8a62856"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.648333 using {'model__neurons': 10}\n",
            "0.500000 (0.000000) with: {'model__neurons': 1}\n",
            "0.500000 (0.000000) with: {'model__neurons': 5}\n",
            "0.648333 (0.209775) with: {'model__neurons': 10}\n",
            "0.500000 (0.000000) with: {'model__neurons': 15}\n",
            "0.500000 (0.000000) with: {'model__neurons': 20}\n",
            "0.500000 (0.000000) with: {'model__neurons': 25}\n",
            "0.500000 (0.000000) with: {'model__neurons': 30}\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(neurons):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(neurons, input_shape=(64*64*3,), activation='hard_sigmoid'))\n",
        " model.add(Dropout(0.1))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " # Compile model\n",
        " optimizer = Nadam(learning_rate=0.001)\n",
        " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=40, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(model__neurons=neurons)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sRij0J79Qy9"
      },
      "source": [
        "Otra opción con la segunda función de activación más eficiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejv3B1bL8I6E",
        "outputId": "eef04333-c76a-4d3e-daa9-db88896c5b90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.815000 using {'model__neurons': 30}\n",
            "0.500000 (0.000000) with: {'model__neurons': 1}\n",
            "0.500000 (0.000000) with: {'model__neurons': 5}\n",
            "0.500000 (0.000000) with: {'model__neurons': 10}\n",
            "0.808333 (0.218034) with: {'model__neurons': 15}\n",
            "0.655000 (0.219203) with: {'model__neurons': 20}\n",
            "0.653333 (0.216846) with: {'model__neurons': 25}\n",
            "0.815000 (0.222748) with: {'model__neurons': 30}\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(neurons):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(neurons, input_shape=(64*64*3,), activation='relu'))\n",
        " model.add(Dropout(0.1))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " # Compile model\n",
        " optimizer = Nadam(learning_rate=0.001)\n",
        " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=40, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(model__neurons=neurons)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_set_x, train_set_y[0])\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
